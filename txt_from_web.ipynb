{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dff27c4f",
   "metadata": {},
   "source": [
    "# Info\n",
    "This notebook contains a bunch of ways to get text from the web that you can write out to text files, which you can then use in markov_modeling.ipynb.\n",
    "\n",
    "The different mini-guides for getting text are:\n",
    "1. Corpora from NLTK\\\n",
    "    a. Brown corpus\\\n",
    "    b. Gutenberg corpus\n",
    "2. Social Media\\\n",
    "    a. Tweets from a specific user \\\n",
    "    b. Transcripts from Youtube\n",
    "3. Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f040ea",
   "metadata": {},
   "source": [
    "# 0. Universal Functions - Run First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a08953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def punctuation_fix(sentence):\n",
    "    '''\n",
    "    Receives a string and returns a string (generally, a single sentence from a larger corpus).\n",
    "    Performs a sequence of removals/substitutions to normalize formatting and fix some common issues.\n",
    "    Note that these changes were largely determined by the files I was using, so you may need to adjust them.\n",
    "    '''\n",
    "    sentence = sentence.replace(\" ,\",\",\").replace(\" .\",\".\").replace(r'(?<=[^:]) \\.\\.\\.',\"...\").replace(\":...\",\": ...\")\\\n",
    "        .replace(\" ?\",\"?\").replace(\" !\",\"!\").replace(\" ;\",\";\").replace(\" :\",\":\").replace(\"  \",\" \").replace(\"   \",\"  \")\\\n",
    "        .replace(\"\\n\",\"\").replace(\" )\", \")\").replace(\"( \",\"(\").replace(\";;\",\";\").replace(\"::\",\":\")\n",
    "    try: \n",
    "        # this regex pattern fixes apostrophe errors for contractions, e.g. \"I 'm\" or \"do n't\"\n",
    "        patterns = [\"[A-Za-z]* '[a-z]\", \"[A-Za-z]* [a-z]'[a-z]\"]\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, sentence)\n",
    "            for match in matches:\n",
    "                match_f = match.replace(\" \",\"\")\n",
    "                sentence = sentence.replace(match, match_f)\n",
    "        # this regex pattern fixes apostrophe errors for questions and exclamations, e.g. \"Hi!It's\" or \"Yes?This\"\n",
    "        odd_pattern = \"[?!](?=[A-Z])\"\n",
    "        matches = re.findall(odd_pattern, sentence)\n",
    "        for match in matches:\n",
    "            sentence = sentence.replace(match, match+\" \")\n",
    "        if sentence[-1] == \" \":\n",
    "            sentence = sentence[0:-1]\n",
    "    except:\n",
    "        pass\n",
    "    return sentence\n",
    "\n",
    "def file_writer(filename, fulltext, overwrite=True):\n",
    "    '''\n",
    "    Receives a filename to write to, a string to write to the file, \n",
    "        and an optional third flag if you want to add to to an existing text instead of overwriting it.\n",
    "    Note that this will create or overwrite a file with the given name, \n",
    "        unless you set overwrite=False when calling the function.\n",
    "    '''\n",
    "    if overwrite == True:\n",
    "        with open(filename, 'w', encoding=\"utf=8\") as f:\n",
    "            f.write(fulltext)\n",
    "    elif overwrite == False:\n",
    "        with open(filename, 'a', encoding=\"utf=8\") as f:\n",
    "            f.write(fulltext+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba708f95",
   "metadata": {},
   "source": [
    "# 0. File Output - Run Last\n",
    "### Set up the various filenames and string names you want to use from the rest of the notebook in the first block, then run the second block to write out the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac6400",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This block defines the file names, strings, and folder structure in preparation for writing out the files.\n",
    "Enter as many tuples as you have strings to write out to a text file; format is (filename, string_name).\n",
    "Note you can give the tuple a third argument, False (no quotes), if you want to extend rather than overwrite the file.\n",
    "'''\n",
    "\n",
    "# folder where you want your output text files saved\n",
    "output_directory = \"data/txts/\"\n",
    "\n",
    "output_files = [\n",
    "    (\"scifi.txt\", scifi_fulltext), \n",
    "    (\"mystery.txt\", mystery_fulltext),\n",
    "    (\"emma.txt\", emma_fulltext, False), \n",
    "    (\"gigi_tweets.txt\", gigi_tweets_fulltext), \n",
    "    (\"gigi_transcripts.txt\", gigi_transcripts_fulltext),\n",
    "    (\"pathologic_script.txt\", pathologic_script),\n",
    "    (\"pathologic_dia.txt\", pathologic_dialogue)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234446d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block writes the files\n",
    "for output in output_files:\n",
    "    if len(output) == 2:\n",
    "        file_writer(output_directory+output[0],output[1])\n",
    "    elif len(output) == 3:\n",
    "        file_writer(output_directory+output[0],output[1], overwrite=output[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2152e021",
   "metadata": {},
   "source": [
    "# 1. NLTK - Brown & Gutenberg\n",
    "### Run next block first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4827ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''if you get an error, add this line to the top of this block:\n",
    "!pip install nltk\n",
    "'''\n",
    "\n",
    "try: \n",
    "    import nltk\n",
    "    from nltk.corpus import brown\n",
    "    from nltk.corpus import gutenberg\n",
    "except:\n",
    "    nltk.download('gutenberg')\n",
    "    import nltk\n",
    "    from nltk.corpus import brown\n",
    "    from nltk.corpus import gutenberg\n",
    "    \n",
    "# fetches and sorts texts for each category in the brown corpus\n",
    "brown_bycat = dict()\n",
    "for category in brown.categories():\n",
    "    sentences = brown.sents(categories=[category])\n",
    "    category_str = \"\"\n",
    "    for sentence in sentences:\n",
    "        sentence_out = \" \".join(sentence)\n",
    "        sentence_out = punctuation_fix(sentence_out)\n",
    "        category_str += sentence_out + \" \"\n",
    "    brown_bycat[category]=category_str\n",
    "\n",
    "# fetches and sorts texts for each category in the gutenberg corpus\n",
    "gutenberg_byid = dict()\n",
    "for fileid in nltk.corpus.gutenberg.fileids():\n",
    "    wordslist = nltk.corpus.gutenberg.words(fileid)\n",
    "    meta_index = wordslist.index(\"]\")\n",
    "    fileid_str = \" \".join(wordslist[meta_index+1:])\n",
    "    fileid_str = punctuation_fix(fileid_str)\n",
    "    gutenberg_byid[fileid] = fileid_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a561d16e",
   "metadata": {},
   "source": [
    "## 1.a Brown\n",
    "brown_bycat is a dictionary we made above, and it maps a category name to a string.\n",
    "The string will be the combined strings of every text in that category, so it'll be pretty long!\n",
    "\n",
    "Categories to choose from: adventure, belles_lettres, editorial, fiction, government, hobbies, humor, learned, lore, mystery, news, religion, reviews, romance, science_fiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1633823",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You can change these variable names to whatever, \n",
    "    just make sure changes are consistent with variable names in 0. File Output.\n",
    "'''\n",
    "scifi_fulltext = brown_bycat[\"science_fiction\"]\n",
    "mystery_fulltext = brown_bycat[\"mystery\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63151d65",
   "metadata": {},
   "source": [
    "## 1.b Gutenberg\n",
    "gutenberg_byid is a dictionary we made above, and it maps a file id to a string. The strings are full books, so be aware of length here too.\n",
    "\n",
    "File ids to choose from: austen-emma.txt, austen-persuasion.txt, austen-sense.txt, bible-kjv.txt, blake-poems.txt, bryant-stories.txt, burgess-busterbrown.txt, carroll-alice.txt, chesterton-ball.txt, chesterton-brown.txt, chesterton-thursday.txt, edgeworth-parents.txt, melville-moby_dick.txt, milton-paradise.txt, shakespeare-caesar.txt, shakespeare-hamlet.txt, shakespeare-macbeth.txt, whitman-leaves.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba355ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You can change this variable name to whatever, \n",
    "    just make sure changes are consistent with the variable names in 0. File Output.\n",
    "'''\n",
    "emma_fulltext = gutenberg_byid[\"austen-emma.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b9a9d",
   "metadata": {},
   "source": [
    "# 2. Social Media - Twitter & YouTube Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4137b92e",
   "metadata": {},
   "source": [
    "## 2.a Twitter\n",
    "### Run next two blocks first, but note!!\n",
    "#### (Note filepath that you'll probably need to update in the first block.)\n",
    "I have this block set up to pull your Twitter access/authorization info from a file, which for me is located in my \"data\" folder. You might need to change this, depending on the location and name of your text file. If you don't know what those are or how to get them, this is a good resource:\n",
    "\n",
    "    https://www.earthdatascience.org/courses/use-data-open-source-python/intro-to-apis/twitter-data-in-python/\n",
    "\n",
    "(Basically, to use Twitter's API, you need to have a certain kind of account and get approval from them by filling out a form. If they approve you, which will (in my experience) happen quickly if you're filling it out as an academic, you'll get the info that is being pulled from the twitter_auth.txt file.\n",
    "\n",
    "Similarly, the file needs to be in this format:\n",
    "\n",
    "    consumer_key='key'\n",
    "    consumer_secret='key'\n",
    "    access_token='key'\n",
    "    access_token_secret='key'\n",
    "\n",
    "\n",
    "Replace the word key in each instance with the correct key, which will be a string of digits, upper-case letters, and lower-case letters. Don't delete the quotation marks or anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b48d4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''if you get an error, add this line to the top of this block:\n",
    "!pip install tweepy\n",
    "'''\n",
    "\n",
    "import tweepy as tw\n",
    "\n",
    "# replace this filepath as needed, or populate the pre-formatted file in the repo\n",
    "twitter_auth = \"data/twitter_auth.txt\"\n",
    "with open(twitter_auth, 'r') as f:\n",
    "    keys = f.readlines()\n",
    "    \n",
    "# converts the pre-formatted file into the variables we need to access the Twitter API via tweepy\n",
    "pattern = r\"(?<=')[-\\w]*(?=')\"\n",
    "consumer_key = re.search(pattern,keys[0]).group(0)\n",
    "consumer_secret = re.search(pattern,keys[1]).group(0)\n",
    "access_token = re.search(pattern,keys[2]).group(0)\n",
    "access_token_secret = re.search(pattern,keys[3]).group(0)\n",
    "\n",
    "# sets up the API\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b2a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_tweets(username, limit):\n",
    "    '''\n",
    "    Receives a sentence and returns a sentence.\n",
    "    Performs a sequence of removals/substitutions to normalize formatting and fix some common issues.\n",
    "    Note that these changes were largely determined by the files I was using, so you may need to adjust them.\n",
    "    Note that if you want each tweet on its own line, change the \" \" part of the join statement to \"\\n\".\n",
    "    '''\n",
    "    tweets = api.user_timeline(screen_name=user, \n",
    "                           count=limit,\n",
    "                           include_rts = False,\n",
    "                           tweet_mode = 'extended'\n",
    "                           )\n",
    "    \n",
    "    tweets_f = list()\n",
    "    for tweet in tweets:\n",
    "        tweet_txt = tweet.full_text\n",
    "        url_rgx = r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'\n",
    "        at_rgx = r'@[a-zA-Z0-9]*'\n",
    "        rgxs = [url_rgx, at_rgx]\n",
    "        for rgx in rgxs:\n",
    "            tweet_txt = re.sub(rgx, \" \", tweet_txt)\n",
    "        tweets_f.append(tweet_txt)\n",
    "    tweets_str = \" \".join(tweets_f)\n",
    "    tweets_str = punctuation_fix(tweets_str)\n",
    "    tweets_str = tweets_str.replace(\"\\n\",\" \")\n",
    "    return tweets_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa15b85",
   "metadata": {},
   "source": [
    "### Enter a username and number of tweets to fetch (max: 900). The result will be a string, so you can add it directly to the File Output section's list toward the top of this notebook. (Plus, if you didn't before, now you know who Gigi Gorgeous is.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd88f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You can change this variable name to whatever, \n",
    "    just make sure changes are consistent with the variable names in 0. File Output.\n",
    "'''\n",
    "user = \"TheGigiGorgeous\"\n",
    "status_count = 100\n",
    "gigi_tweets_fulltext = user_tweets(user, status_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6088f8b",
   "metadata": {},
   "source": [
    "## 2.b Youtube Transcripts\n",
    "### Run next two blocks first, but note!!\n",
    "#### (Note filepath that you'll probably need to update in the first block.)\n",
    "The way this API works is that it relies on video IDs. The ID is visible from the URL:\n",
    "\n",
    "    url: https://www.youtube.com/watch?v=LxOUh1qO8Ls\n",
    "    id: LxOUh1qO8Ls\n",
    "    (don't include & or any other extra stuff after the id)\n",
    "    \n",
    "To avoid extra work, I've made it so that if you save a txt file where each line in the file is a full Youtube link like the one above (*just* the link), then it'll automatically turn those links into the necessary shortened ids. That way, if you have a link extractor in your web browser (e.g. Link Gopher for Firefox), you can just open a playlist or channel's page of videos in your browser, run the link extractor, and copy+paste all the relevant links into a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e467db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''if you get an error, add this line to the top of this block:\n",
    "!pip install youtube_transcript_api\n",
    "'''\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "# replace this filepath as needed, or populate the pre-formatted file in the repo; note you can do ids or full links\n",
    "videourls_path = \"data/video_ids.txt\"\n",
    "\n",
    "\n",
    "# takes lines from the file and convert them into the needed ids for scraping transcripts using the API\n",
    "with open(videourls_path, 'r') as f:\n",
    "    videourls = f.readlines()\n",
    "\n",
    "videoids = list()\n",
    "pattern = r\"(?<=\\?v=)[\\w\\_-]*\"\n",
    "for url in videourls:\n",
    "    try: \n",
    "        videoid = re.search(pattern,url).group(0)\n",
    "        videoids.append(videoid)\n",
    "    except:\n",
    "        videoids.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_transcripts(videoids_list):\n",
    "    '''\n",
    "    Receives a list of videoids and returns a string.\n",
    "    Goes through every video in the list of ids, fetches the transcript, grabs the text, \n",
    "        performs some sentence tidying, and then adds it to the larger string.\n",
    "    The string it returns combines all the text from the transcripts for every video in the id list.\n",
    "    Note that this fetches English transcripts (favors manually made over auto-generated), \n",
    "        but you can change the language.\n",
    "    '''\n",
    "    transcripts = \"\"\n",
    "    for videoid in videoids_list:\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(videoid, languages=['en'])\n",
    "            transcript_str = \"\"\n",
    "            for line in transcript:\n",
    "                transcript_str += line['text']\n",
    "                transcript_str = re.sub(r\"\\[.*\\]\", \"\",transcript_str)\n",
    "                transcript_str = transcript_str.replace(\" i \", \" I \").replace(\" i'\", \" I'\")\n",
    "                transcript_str = punctuation_fix(transcript_str) + \" \"\n",
    "            transcripts += transcript_str\n",
    "        except:\n",
    "            pass\n",
    "    return transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288f6dec",
   "metadata": {},
   "source": [
    "### Since you're doing most of the decision-making when you make the .txt file, you're mostly just running this next block and copying the variable name for the file output section at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859b207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You can change this variable name to whatever, \n",
    "    just make sure changes are consistent with the variable names in 0. File Output.\n",
    "'''\n",
    "gigi_transcripts_fulltext = video_transcripts(videoids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd37994",
   "metadata": {},
   "source": [
    "# 3. Webpages\n",
    "### Run next two blocks first\n",
    "Note that this probably won't be useful for most websites since each is formatted differently, but the soup-ification process is all set up, so you can tinker with it as needed. The next two blocks will work for any website, since they just do the necessary prereq imports and define some functions, but note that any url besides urls_to_soups makes some basic assumptions about the structure of a page. You might need to modify them based on the structure of the page you're looking at and what you're trying to get. If you want to learn more, this might be useful: \n",
    "\n",
    "    https://www.analyticsvidhya.com/blog/2021/08/a-simple-introduction-to-web-scraping-with-beautiful-soup/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8cd8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''if you get an error, add this line to the top of this block:\n",
    "!pip install BeautifulSoup\n",
    "'''\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ddae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def urls_to_soups(urls):\n",
    "    '''\n",
    "    Receives a list of urls and returns a list of soups.\n",
    "    Note that the returned list will be of the same length as the input list, and the indexes should line up.\n",
    "    '''\n",
    "    soups = list()\n",
    "    for url in urls:\n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "        soups.append(soup)\n",
    "    return soups\n",
    "\n",
    "def urls_from_sitemap(sitemap_soups, root_url):\n",
    "    '''\n",
    "    Receives a list of soups and a root url, and returns a list of urls.\n",
    "    The input soups should be for the sitemaps of the site, and the root url should be, well, the root url.\n",
    "        (See block below for more info on root urls.)\n",
    "    The returned list will grab all the urls on that page as per certain criteria (see note within the for loop below).\n",
    "    '''\n",
    "    hrefs_txt = list()\n",
    "    for sitemap_soup in sitemap_soups:\n",
    "        ahrefs = sitemap_soup.find_all('a')\n",
    "        for ahref in ahrefs:\n",
    "            href_txt = ahref.get('href')\n",
    "            '''\n",
    "            The lines from the if statement until the return statement are what you'll need to change based on\n",
    "            the structure of the website; for example, this only grabs links from the sitemap page that \n",
    "            start with \"html_en\" or \"html2_en\", because those strings were unique to the links I wanted to \n",
    "            grab from the page. The last two statements before the return were manual removal of unwanted links.\n",
    "            \n",
    "            Eventually, I'll make the qualifiers more flexible by making them function parameters.\n",
    "            '''\n",
    "            if href_txt[0:7] == \"html_en\" or  href_txt[0:8] == \"html2_en\":\n",
    "                hrefs_txt.append(root_url+href_txt)\n",
    "        hrefs_txt.pop(91)\n",
    "        hrefs_txt = hrefs_txt[:-3]\n",
    "        \n",
    "    # this will stay the same\n",
    "    return hrefs_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081081ef",
   "metadata": {},
   "source": [
    "### ***Run the next block only once!***\n",
    "\n",
    "It makes requests to the pages, and if you do it a bunch then you might annoy them or get rate limited (which means you're prevented from making more requests; it's also rude, imo). The result will be a list of soups, where each item is a soup corresponding to the urls pulled from the sitemap via the function.\n",
    "\n",
    "You should also double check the rules of the site for web crawling. You can find these rules for most sites by taking the root url (everything up to the top-level domain, e.g. https://www.wikipedia.org or https://github.com) and adding \"/robots.txt\" (no quotation marks). \n",
    "\n",
    "For example:\n",
    "\n",
    "     https://www.wikipedia.org/robots.txt\n",
    "     https://github.com/robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c40dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note:\n",
    "    * sitemap_urls is a list even if there's only one item\n",
    "    * the root url includes a / at the end; this is important!!\n",
    "    * you can populate the sitemap_urls list with whatever you need based on the structure of the site you're using\n",
    "'''\n",
    "root = \"https://pathologicdialogue.github.io/\"\n",
    "sitemap_urls = [\"https://pathologicdialogue.github.io/\"]\n",
    "\n",
    "# converts url(s) for the sitemap page(s) into soup objects; includes request to server\n",
    "sitemap_soups = urls_to_soups(sitemap_urls)\n",
    "\n",
    "# grabs all the urls on the sitemap pages as per the function; does not include request to server\n",
    "subpage_urls = urls_from_sitemap(sitemap_soups, root)\n",
    "\n",
    "# converts urls for the fetched urls into soup objects; includes request to server\n",
    "subpage_soups = urls_to_soups(subpage_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b600dd05",
   "metadata": {},
   "source": [
    "### Okay, sorry for yelling up there with the italics. You're good from here down.\n",
    "\n",
    "In fact, you'll have to change and tinker with a lot of this stuff. This is the stuff I used to parse the very specific pages I pulled from the site above. It will almost assuredly not be directly useful for your needs, but it's here for two reasons:\n",
    "\n",
    "1. I'm still using this notebook, y'know.\n",
    "2. Maybe seeing how I got text from a pretty idiosyncratically formatted site can help you see some methods for parsing soup objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb64bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soups_to_sentences(soups, urls):.\n",
    "    '''\n",
    "    Receives a list of soups and the associated list of urls (so urls[5] is the url for soups[5], \n",
    "        and returns a list of sentences.\n",
    "    The input soups should be the fetched pages from the sitemap that you plan to scrape for content.\n",
    "    Note that you'll have to change almost all of this based on the site and page structure of your webcraweld pages.\n",
    "    '''\n",
    "    sentences = list()\n",
    "    for idx, soup in enumerate(soups):\n",
    "        if \"html_\" in urls[idx]:\n",
    "            maindivs = soup.find_all('div', {\"class\": \"ui-content\"})\n",
    "            for maindiv in maindivs:\n",
    "                subdivs = maindiv.find_all('div', recursive=False)\n",
    "                for div in subdivs:\n",
    "                    paras = div.find_all('p')\n",
    "                    for para in paras:\n",
    "                        para_f = para.text\n",
    "                        para_f = para_f.split(\".\", 1)\n",
    "                        para_f = para_f[-1]\n",
    "                        para_f = para_f.replace(\"<\",\"\").replace(\">\",\"\").replace(\"/\",\"\").replace(\"\\\\\",\"\").replace(\"RatProphet_speech_1\",\"Rat Prophet\")\n",
    "                        sentences.append(para_f)\n",
    "                    sentences.append(\"\\n\")\n",
    "        elif \"html2_\" in urls[idx]:\n",
    "            maindivs = soup.find_all('script')\n",
    "            pattern = r'textarr=\\[.*\\]'\n",
    "            subpattern = r'(?<=\\[)(.*?)(?=[\\\"\\']\\])'\n",
    "            for maindiv in maindivs:\n",
    "                try: \n",
    "                    m_arr = re.search(pattern,str(maindiv))\n",
    "                    match_arr = m_arr.group(0)\n",
    "                    m_cont = re.search(subpattern, match_arr)\n",
    "                    match_cont = m_cont.group(0)\n",
    "                    match_cont = match_cont.replace(\"<\",\"\").replace(\">\",\"\").replace(\"/\",\"\").replace(\"\\\\\",\"\").replace(\"RatProphet_speech_1\",\"Rat Prophet\")\n",
    "                    cont_arr = re.split('''['\"], ['\"]''',match_cont)\n",
    "                except:\n",
    "                    pass\n",
    "            sentences.extend(cont_arr)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def sentences_to_fulltexts(sentences):\n",
    "    '''\n",
    "    Receives a list of sentences and returns two strings: \n",
    "        1) everything in the sentence list\n",
    "        2) just the dialogue from the sentence list without the preceding character tags\n",
    "    '''\n",
    "    script = \"\"\n",
    "    dialogue = \"\"\n",
    "    for line in sentences:\n",
    "        if line != None:\n",
    "            script += line + \"\\n\"\n",
    "            try: \n",
    "                character, statement = line.split(\": \", 1)\n",
    "                dialogue += statement + \" \"\n",
    "            except:\n",
    "                pass\n",
    "    return script, dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8abe9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the soups into sentences that will be used to build the strings below\n",
    "pathologic_script_sentences = soups_to_sentences(subpage_soups, subpage_urls)\n",
    "\n",
    "'''\n",
    "You can change these variable names to whatever, \n",
    "    just make sure changes are consistent with the variable names in 0. File Output.\n",
    "'''\n",
    "pathologic_script, pathologic_dialogue = sentences_to_fulltexts(pathologic_script_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
