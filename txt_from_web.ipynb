{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7aa86ec",
   "metadata": {},
   "source": [
    "##### What follows is a bunch of ways to get text from the web that you can write out to files. It includes guides for pulling some basic text from various places on the web. The main ones:\n",
    "\n",
    "1. Corpora from NLTK\n",
    "    a. Brown corpus\n",
    "    b. Gutenberg corpus\n",
    "2. Social Media\n",
    "    a. Tweets from a specific user\n",
    "    b. Transcripts from Youtube\n",
    "3. Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f040ea",
   "metadata": {},
   "source": [
    "# 0. Universal Functions - Run First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a08953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# i made this specifically for the original text file I was using, but it can catch a lot of weird issues with spaces\n",
    "# note that it runs at the sentence level, rather than at the level of a full text\n",
    "def punctuation_fix(sentence):\n",
    "    sentence = sentence.replace(\" ,\",\",\").replace(\" .\",\".\").replace(r'(?<=[^:]) \\.\\.\\.',\"...\").replace(\":...\",\": ...\").replace(\" ?\",\"?\").replace(\" !\",\"!\").replace(\" ;\",\";\").replace(\" :\",\":\").replace(\"  \",\" \").replace(\"   \",\"  \").replace(\"\\n\",\"\").replace(\" )\", \")\").replace(\"( \",\"(\").replace(\";;\",\";\").replace(\"::\",\":\")\n",
    "    try: \n",
    "        # this regex pattern fixes apostrophe errors for contractions, e.g. \"I 'm\" or \"do n't\"\n",
    "        patterns = [\"[A-Za-z]* '[a-z]\", \"[A-Za-z]* [a-z]'[a-z]\"]\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, sentence)\n",
    "            for match in matches:\n",
    "                match_f = match.replace(\" \",\"\")\n",
    "                sentence = sentence.replace(match, match_f)\n",
    "        # this regex pattern fixes apostrophe errors for questions and exclamations, e.g. \"Hi!It's\" or \"Yes?This\"\n",
    "        odd_pattern = \"[?!](?=[A-Z])\"\n",
    "        matches = re.findall(odd_pattern, sentence)\n",
    "        for match in matches:\n",
    "            sentence = sentence.replace(match, match+\" \")\n",
    "        if sentence[-1] == \" \":\n",
    "            sentence = sentence[0:-1]\n",
    "    except:\n",
    "        pass\n",
    "    return sentence\n",
    "\n",
    "# writes out a file given a name; \n",
    "# NOTE! if you want to extend a file rather than make it from scratch, be sure to include\n",
    "# overwrite=False as a third input when you call the function; otherwise it'll write over your file\n",
    "def file_writer(filename, fulltext, overwrite=True):\n",
    "    if overwrite == True:\n",
    "        with open(filename, 'w', encoding=\"utf=8\") as f:\n",
    "            f.write(fulltext)\n",
    "    elif overwrite == False:\n",
    "        with open(filename, 'a', encoding=\"utf=8\") as f:\n",
    "            f.write(fulltext+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba708f95",
   "metadata": {},
   "source": [
    "# 0. File Output - Run Last\n",
    "### Set up the various filenames and string names you want to use from the rest of the notebook in the first block, then run the second block to write out the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac6400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter as many tuples as you have strings to write out to a text file; format is (filename, string_name)\n",
    "# note you can give the tuple a third argument, False (no quotes), if you want to extend rather than overwrite the file\n",
    "output_files = [\n",
    "    (\"scifi.txt\", scifi_fulltext), \n",
    "    (\"mystery.txt\", mystery_fulltext),\n",
    "    (\"emma.txt\", emma_fulltext, False), \n",
    "    (\"gigi_tweets.txt\", gigi_tweets_fulltext), \n",
    "    (\"gigi_transcripts.txt\", gigi_transcripts_fulltext),\n",
    "    (\"pathologic_script.txt\", pathologic_script),\n",
    "    (\"pathologic_dia.txt\", pathologic_dialogue)\n",
    "    ]\n",
    "\n",
    "# folder where you want your output text files saved\n",
    "output_directory = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234446d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in output_files:\n",
    "    if len(output) == 2:\n",
    "        file_writer(output_directory+output[0],output[1])\n",
    "    elif len(output) == 3:\n",
    "        file_writer(output_directory+output[0],output[1], overwrite=output[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2152e021",
   "metadata": {},
   "source": [
    "# 1. NLTK - Brown & Gutenberg\n",
    "### Run next block first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4827ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''if needed:\n",
    "!pip install nltk\n",
    "nltk.download('gutenberg')\n",
    "'''\n",
    "# run this; it will import the brown and gutenberg corpora tools, and build a dictionary we'll use for the brown corpus\n",
    "try: \n",
    "    import nltk\n",
    "    from nltk.corpus import brown\n",
    "    from nltk.corpus import gutenberg\n",
    "except:\n",
    "    nltk.download('gutenberg')\n",
    "    import nltk\n",
    "    from nltk.corpus import brown\n",
    "    from nltk.corpus import gutenberg\n",
    "    \n",
    "brown_bycat = dict()\n",
    "gutenberg_byid = dict()\n",
    "\n",
    "# fetching and sorting texts for each category in the brown corpus\n",
    "for category in brown.categories():\n",
    "    sentences = brown.sents(categories=[category])\n",
    "    category_str = \"\"\n",
    "    for sentence in sentences:\n",
    "        sentence_out = \" \".join(sentence)\n",
    "        sentence_out = punctuation_fix(sentence_out)\n",
    "        category_str += sentence_out + \" \"\n",
    "    brown_bycat[category]=category_str\n",
    "\n",
    "# fetching and sorting texts for each category in the gutenberg corpus\n",
    "for fileid in nltk.corpus.gutenberg.fileids():\n",
    "    wordslist = nltk.corpus.gutenberg.words(fileid)\n",
    "    meta_index = wordslist.index(\"]\")\n",
    "    fileid_str = \" \".join(wordslist[meta_index+1:])\n",
    "    fileid_str = punctuation_fix(fileid_str)\n",
    "    gutenberg_byid[fileid] = fileid_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a561d16e",
   "metadata": {},
   "source": [
    "## 1.a Brown\n",
    "brown_bycat is a dictionary we made above, and it maps a category name to a string.\n",
    "The string will be the combined strings of every text in that category, so it'll be pretty long!\n",
    "\n",
    "Categories to choose from: adventure, belles_lettres, editorial, fiction, government, hobbies, humor, learned, lore, mystery, news, religion, reviews, romance, science_fiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1633823",
   "metadata": {},
   "outputs": [],
   "source": [
    "scifi_fulltext = brown_bycat[\"science_fiction\"]\n",
    "mystery_fulltext = brown_bycat[\"mystery\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63151d65",
   "metadata": {},
   "source": [
    "## 1.b Gutenberg\n",
    "gutenberg_byid is a dictionary we made above, and it maps a file id to a string. The strings are full books, so be aware of length here too.\n",
    "\n",
    "File ids to choose from: austen-emma.txt, austen-persuasion.txt, austen-sense.txt, bible-kjv.txt, blake-poems.txt, bryant-stories.txt, burgess-busterbrown.txt, carroll-alice.txt, chesterton-ball.txt, chesterton-brown.txt, chesterton-thursday.txt, edgeworth-parents.txt, melville-moby_dick.txt, milton-paradise.txt, shakespeare-caesar.txt, shakespeare-hamlet.txt, shakespeare-macbeth.txt, whitman-leaves.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba355ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_fulltext = gutenberg_byid[\"austen-emma.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b9a9d",
   "metadata": {},
   "source": [
    "# 2. Social Media - Twitter & YouTube Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4137b92e",
   "metadata": {},
   "source": [
    "## 2.a Twitter\n",
    "### Run next two blocks first, but note!!\n",
    "#### (Note filepath that you'll probably need to update in the first block.)\n",
    "I have this block set up to pull your Twitter access/authorization info from a file, which for me is located in my \"data\" folder. You might need to change this, depending on the location and name of your text file. If you don't know what those are or how to get them, this is a good resource:\n",
    "\n",
    "    https://www.earthdatascience.org/courses/use-data-open-source-python/intro-to-apis/twitter-data-in-python/\n",
    "\n",
    "(Basically, to use Twitter's API, you need to have a certain kind of account and get approval from them by filling out a form. If they approve you, which will (in my experience) happen quickly if you're filling it out as an academic, you'll get the info that is being pulled from the twitter_auth.txt file.\n",
    "\n",
    "Similarly, the file needs to be in this format:\n",
    "\n",
    "    consumer_key='key'\n",
    "    consumer_secret='key'\n",
    "    access_token='key'\n",
    "    access_token_secret='key'\n",
    "\n",
    "\n",
    "Replace the word key in each instance with the correct key, which will be a string of digits, upper-case letters, and lower-case letters. Don't delete the quotation marks or anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b48d4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''if needed:\n",
    "!pip install tweepy\n",
    "'''\n",
    "import tweepy as tw\n",
    "\n",
    "twitter_auth = \"data/twitter_auth.txt\"\n",
    "with open(twitter_auth, 'r') as f:\n",
    "    keys = f.readlines()\n",
    "    \n",
    "pattern = r\"(?<=')[-\\w]*(?=')\"\n",
    "consumer_key = re.search(pattern,keys[0]).group(0)\n",
    "consumer_secret = re.search(pattern,keys[1]).group(0)\n",
    "access_token = re.search(pattern,keys[2]).group(0)\n",
    "access_token_secret = re.search(pattern,keys[3]).group(0)\n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b2a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns a username and number of tweets to fetch into a string, with tweets separated by spaces\n",
    "# if you want each tweet on its own line, change the \" \" part of the join statement to \"\\n\"\n",
    "def user_tweets(username, limit):\n",
    "    tweets = api.user_timeline(screen_name=user, \n",
    "                           count=limit,\n",
    "                           include_rts = False,\n",
    "                           tweet_mode = 'extended'\n",
    "                           )\n",
    "    \n",
    "    tweets_f = list()\n",
    "    for tweet in tweets:\n",
    "        tweet_txt = tweet.full_text\n",
    "        url_rgx = r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'\n",
    "        at_rgx = r'@[a-zA-Z0-9]*'\n",
    "        rgxs = [url_rgx, at_rgx]\n",
    "        for rgx in rgxs:\n",
    "            tweet_txt = re.sub(rgx, \" \", tweet_txt)\n",
    "        tweets_f.append(tweet_txt)\n",
    "    tweets_str = \" \".join(tweets_f)\n",
    "    tweets_str = punctuation_fix(tweets_str)\n",
    "    tweets_str = tweets_str.replace(\"\\n\",\" \")\n",
    "    return tweets_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa15b85",
   "metadata": {},
   "source": [
    "### Enter a username and number of tweets to fetch (max: 900). The result will be a string, so you can add it directly to the File Output section's list toward the top of this notebook. (Plus, if you didn't before, now you know who Gigi Gorgeous is.) You can call the fulltext variable whatever you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd88f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"TheGigiGorgeous\"\n",
    "status_count = 100\n",
    "gigi_tweets_fulltext = user_tweets(user, status_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6088f8b",
   "metadata": {},
   "source": [
    "## 2.b Youtube Transcripts\n",
    "### Run next two blocks first, but note!!\n",
    "#### (Note filepath that you'll probably need to update in the first block.)\n",
    "The way this API works is that it relies on video IDs. The ID is visible from the URL:\n",
    "\n",
    "    url: https://www.youtube.com/watch?v=LxOUh1qO8Ls\n",
    "    id: LxOUh1qO8Ls\n",
    "    (don't include & or any other extra stuff after the id)\n",
    "    \n",
    "To avoid extra work, I've made it so that if you save a txt file where each line in the file is a full Youtube link like the one above (*just* the link), then it'll automatically turn those links into the necessary shortened ids. That way, if you have a link extractor in your web browser (e.g. Link Gopher for Firefox), you can just open a playlist or channel's page of videos in your browser, run the link extractor, and copy+paste all the relevant links into a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e467db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''if needed:\n",
    "!pip install youtube_transcript_api\n",
    "'''\n",
    "# run this; it will import a Youtube transcript scraper\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "videourls_path = \"data/video_ids.txt\"\n",
    "\n",
    "with open(videourls_path, 'r') as f:\n",
    "    videourls = f.readlines()\n",
    "\n",
    "videoids = list()\n",
    "pattern = r\"(?<=\\?v=)[\\w\\_-]*\"\n",
    "for url in videourls:\n",
    "    try: \n",
    "        videoid = re.search(pattern,url).group(0)\n",
    "        videoids.append(videoid)\n",
    "    except:\n",
    "        videoids.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns a list of video ids (see code and markdown blocks above) into a text string of those videos' eng transcripts\n",
    "def video_transcripts(videoids_list):\n",
    "    transcripts = \"\"\n",
    "    for videoid in videoids_list:\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(videoid, languages=['en'])\n",
    "            transcript_str = \"\"\n",
    "            for line in transcript:\n",
    "                transcript_str += line['text']\n",
    "                transcript_str = re.sub(r\"\\[.*\\]\", \"\",transcript_str)\n",
    "                transcript_str = transcript_str.replace(\" i \", \" I \").replace(\" i'\", \" I'\")\n",
    "                transcript_str = punctuation_fix(transcript_str) + \" \"\n",
    "            transcripts += transcript_str\n",
    "        except:\n",
    "            pass\n",
    "    return transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288f6dec",
   "metadata": {},
   "source": [
    "### Since you're doing most of the decision-making when you make the .txt file, you're mostly just running this next block and copying the variable name for the file output section at the top. You can call the fulltext variable whatever you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859b207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gigi_transcripts_fulltext = video_transcripts(videoids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd37994",
   "metadata": {},
   "source": [
    "# 3. Webpages\n",
    "### Run next two blocks first\n",
    "Note that this probably won't be useful for most websites since each is formatted differently, but the soup-ification process is all set up, so you can tinker with it as needed. The next two blocks will work for any website, since they just do the necessary prereq imports and define some functions, but note that any url besides urls_to_soups makes some basic assumptions about the structure of a page. You might need to modify them based on the structure of the page you're looking at and what you're trying to get. If you want to learn more, this might be useful: \n",
    "\n",
    "    https://www.analyticsvidhya.com/blog/2021/08/a-simple-introduction-to-web-scraping-with-beautiful-soup/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8cd8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''if needed:\n",
    "!pip BeautifulSoup\n",
    "'''\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ddae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def urls_to_soups(urls):\n",
    "    soups = list()\n",
    "    for url in urls:\n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "        soups.append(soup)\n",
    "    return soups\n",
    "\n",
    "def urls_from_sitemap(sitemap_soups, root_urls):\n",
    "    hrefs_txt = list()\n",
    "    for sitemap_soup in sitemap_soups:\n",
    "        ahrefs = sitemap_soup.find_all('a')\n",
    "        for ahref in ahrefs:\n",
    "            href_txt = ahref.get('href')\n",
    "            '''\n",
    "            the lines from the if statement until the return statement are what you'll need to change based on\n",
    "            the structure of the website; for example, this only grabs links from the sitemap page that \n",
    "            start with \"html_en\" or \"html2_en\", because those strings were unique to the links I wanted to \n",
    "            grab from the page. The last two statements before the return were manual removal of unwanted links.\n",
    "            \n",
    "            Eventually, I'll make the qualifiers more flexible by making them function parameters.\n",
    "            '''\n",
    "            if href_txt[0:7] == \"html_en\" or  href_txt[0:8] == \"html2_en\":\n",
    "                hrefs_txt.append(root_url+href_txt)\n",
    "        hrefs_txt.pop(91)\n",
    "        hrefs_txt = hrefs_txt[:-3]    \n",
    "    return hrefs_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081081ef",
   "metadata": {},
   "source": [
    "### ***Run the next block only once!***\n",
    "\n",
    "It makes requests to the pages, and if you do it a bunch then you might annoy them or get rate limited (which means you're prevented from making more requests; it's also rude, imo). The result will be a list of soups, where each item is a soup corresponding to the urls pulled from the sitemap via the function.\n",
    "\n",
    "You should also double check the rules of the site for web crawling. You can find these rules for most sites by taking the root url (everything up to the top-level domain, e.g. https://www.wikipedia.org or https://github.com) and adding \"/robots.txt\" (no quotation marks). \n",
    "\n",
    "For example:\n",
    "\n",
    "     https://www.wikipedia.org/robots.txt\n",
    "     https://github.com/robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c40dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "note:\n",
    "    * sitemap_urls is a list even if there's only one item\n",
    "    * the root url includes a / at the end; this is important!\n",
    "    * you can populate the sitemap_urls list with whatever you need based on the structure of the site you're using\n",
    "'''\n",
    "root_url = \"https://pathologicdialogue.github.io/\"\n",
    "sitemap_urls = [\"https://pathologicdialogue.github.io/\"]\n",
    "\n",
    "# note that sitemap_soups is a list that is the same length as sitemap_urls\n",
    "sitemap_soups = urls_to_soups(sitemap_urls)\n",
    "\n",
    "# here we grab all the urls on the sitemap pages as per the function, then turn them into an equal-length list of soups\n",
    "subpage_urls = urls_from_sitemap(sitemap_soups, root_url)\n",
    "subpage_soups = urls_to_soups(subpage_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b600dd05",
   "metadata": {},
   "source": [
    "### Okay, sorry for yelling up there with the italics. You're good from here down.\n",
    "\n",
    "In fact, you'll have to change and tinker with a lot of this stuff. This is the stuff I used to parse the very specific pages I pulled from the site above. It will almost assuredly not be directly useful for your needs, but it's here for two reasons:\n",
    "\n",
    "1. I'm still using this notebook, y'know.\n",
    "2. Maybe seeing how I got text from a pretty idiosyncratically formatted site can help you see some methods for parsing soup objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb64bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soups_to_sentences(soups, urls):\n",
    "    sentences = list()\n",
    "    for idx, soup in enumerate(soups):\n",
    "        if \"html_\" in urls[idx]:\n",
    "            maindivs = soup.find_all('div', {\"class\": \"ui-content\"})\n",
    "            for maindiv in maindivs:\n",
    "                subdivs = maindiv.find_all('div', recursive=False)\n",
    "                for div in subdivs:\n",
    "                    paras = div.find_all('p')\n",
    "                    for para in paras:\n",
    "                        para_f = para.text\n",
    "                        para_f = para_f.split(\".\", 1)\n",
    "                        para_f = para_f[-1]\n",
    "                        para_f = para_f.replace(\"<\",\"\").replace(\">\",\"\").replace(\"/\",\"\").replace(\"\\\\\",\"\").replace(\"RatProphet_speech_1\",\"Rat Prophet\")\n",
    "                        sentences.append(para_f)\n",
    "                    sentences.append(\"\\n\")\n",
    "        elif \"html2_\" in urls[idx]:\n",
    "            maindivs = soup.find_all('script')\n",
    "            pattern = r'textarr=\\[.*\\]'\n",
    "            subpattern = r'(?<=\\[)(.*?)(?=[\\\"\\']\\])'\n",
    "            for maindiv in maindivs:\n",
    "                try: \n",
    "                    m_arr = re.search(pattern,str(maindiv))\n",
    "                    match_arr = m_arr.group(0)\n",
    "                    m_cont = re.search(subpattern, match_arr)\n",
    "                    match_cont = m_cont.group(0)\n",
    "                    match_cont = match_cont.replace(\"<\",\"\").replace(\">\",\"\").replace(\"/\",\"\").replace(\"\\\\\",\"\").replace(\"RatProphet_speech_1\",\"Rat Prophet\")\n",
    "                    cont_arr = re.split('''['\"], ['\"]''',match_cont)\n",
    "                except:\n",
    "                    pass\n",
    "            sentences.extend(cont_arr)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def sentences_to_fulltexts(sentences):\n",
    "    script = \"\"\n",
    "    dialogue = \"\"\n",
    "    for line in sentences:\n",
    "        if line != None:\n",
    "            script += line + \"\\n\"\n",
    "            try: \n",
    "                character, statement = line.split(\": \", 1)\n",
    "                dialogue += statement + \" \"\n",
    "            except:\n",
    "                pass\n",
    "    return script, dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8abe9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathologic_script_sentences = soups_to_sentences(subpage_soups, subpage_urls)\n",
    "pathologic_script, pathologic_dialogue = sentences_to_fulltexts(pathologic_script_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
